# Kafka Avro Orders — Producer & Consumer

A compact Python-based Kafka demo showcasing Avro serialization (with `fastavro`), the Confluent Kafka client, message failure handling with retries and a Dead Letter Queue (DLQ), and simple aggregation of order prices.

This repository contains a small producer that generates randomized orders and a consumer that deserializes Avro messages, applies failure rules, retries when appropriate, and funnels permanently failing messages to an `orders-dlq` topic with helpful metadata.

Note: this project was developed on Ubuntu Linux and tested with Kafka running in KRaft mode (no ZooKeeper).

## Screenshots

Place your screenshots in the `Documents/` folder. Example filenames used in this README (replace with your actual screenshots):

- `Documents/producer-send.png` — Producer run / delivery reports
- `Documents/consumer-run.png` — Consumer console output showing processing
- `Documents/dlq-messages.png` — Inspection of `orders-dlq` topic showing headers

Image gallery (ordered for a logical walkthrough):

<figure>
  <img width="1920" height="966" alt="Successfully run kafka server in linux environment" src="https://github.com/user-attachments/assets/070e8666-babb-4831-94a9-c8897cbbec02" />
  <figcaption>1) Kafka server successfully started (server terminal output).</figcaption>
</figure>

<figure>
  <img width="1920" height="966" alt="Create Order topic in kafka" src="https://github.com/user-attachments/assets/82236825-1dfb-40b4-a6b6-adcf35dca927" />
  <figcaption>2) Creating the `orders` topic in Kafka.</figcaption>
</figure>

<figure>
  <img width="1920" height="966" alt="Generate 15 Order messages under Order topic" src="https://github.com/user-attachments/assets/dd9fbb84-0656-4546-9aac-264366c98082" />
  <figcaption>3) Producer run generating 15 order messages to the `orders` topic.</figcaption>
</figure>

<figure>
  <img width="1920" height="966" alt="Consumer can see the Orders generated by producer" src="https://github.com/user-attachments/assets/d5ad5f35-ce8e-4cb4-bd61-8be92c93cdcb" />
  <figcaption>4) Consumer console output showing processing of messages and retry/DLQ behaviour.</figcaption>
</figure>

<figure>
  <img width="1920" height="966" alt="DLQ terminal successfully running" src="https://github.com/user-attachments/assets/4ca887e9-bf99-4517-a8c7-2a85884cb2ed" />
  <figcaption>5) DLQ (`orders-dlq`) inspection / DLQ consumer showing failed messages and headers.</figcaption>
</figure>

## Quick summary

- Producer: `producer.py` — generates 15 random orders, serializes each using Avro (`order.avsc`) via `fastavro`, and sends them to the `orders` Kafka topic.
- Consumer: `consumer.py` — subscribes to `orders`, deserializes Avro messages, enforces failure rules, retries transient failures, and routes permanent failures to `orders-dlq` with headers explaining the reason.
- Schema: `order.avsc` — Avro schema describing an `Order` with `orderId`, `product`, and `price`.

## Technologies used

- Python 3 (tested with Python 3.12 in this workspace)
- Apache Kafka (local broker used for testing; tested in KRaft mode)
- Avro serialization via `fastavro`
- Confluent Kafka Python client: `confluent-kafka`
- Faker (for random product names)
- Standard libs: `json`, `uuid`, `random`, `io`, `time`

## Required pip packages

Install the runtime dependencies (recommended inside a virtual environment):

```bash
# if you prefer a fresh venv
python3 -m venv venv
source venv/bin/activate

pip install --upgrade pip
pip install -r requirements.txt
```

Note: On some platforms `confluent-kafka` may require system libraries (librdkafka). On Debian/Ubuntu:

```bash
sudo apt-get update
sudo apt-get install -y librdkafka-dev
```

If you prefer a `requirements.txt` from current environment:

```bash
pip freeze > requirements.txt
```

## Files in this repo

- `producer.py` — Avro-based Kafka producer that generates and sends sample orders.
- `consumer.py` — Avro-based Kafka consumer that enforces failure rules, retries, aggregates results, and sends permanent failures to DLQ.
- `order.avsc` — Avro schema used by producer and consumer.
- `Documents/` — Suggested place for screenshots used in this README.

## How it works — theory and implementation notes

1. Avro serialization
   - The producer reads `order.avsc` and uses `fastavro.parse_schema` + `fastavro.writer` to serialize Python dicts into Avro bytes. This provides compact, schema-driven serialization and ensures both producer and consumer agree on the record layout.

2. Confluent Kafka client
   - `confluent-kafka` provides high-performance producers and consumers backed by librdkafka. The producer uses `Producer.produce()` and polls for delivery reports. The consumer uses `Consumer.poll()` and manual offset handling to support retry semantics.

3. Consumer behavior, retry rules, and DLQ
   - The consumer disables auto commits (`enable.auto.commit: False`) and performs manual commit logic to allow messages to be retried or sent to a DLQ when needed.
   - Failure rules implemented in `consumer.py`:
     - Transient failure: `5.0 <= price <= 50.0` — these are considered transient and will be retried (not committed) so they can be reprocessed.
     - Permanent failure: `price > 1000.0` — these are permanently failed and are forwarded to the DLQ topic `orders-dlq` with metadata headers.
   - DLQ headers include useful metadata fields: `error_reason`, `original_topic`, `original_partition`, `original_offset`, and `timestamp`. The raw failing message bytes are used as DLQ message `value` so the original payload is preserved.
   - Retry logic: consumer implements a retry loop (`max_retries` configurable) — if processing consistently fails beyond this threshold the message is sent to DLQ.

4. Aggregation and stats
   - The consumer keeps running aggregations (total price, order count, average) and prints summaries when interrupted. This demonstrates how stateful processing/metrics can be collected alongside per-message failure handling.

## Setup & running steps

1) Start Kafka (local broker, KRaft mode / no ZooKeeper)

- This project was developed using Kafka in KRaft mode (no ZooKeeper). Start your Kafka broker according to KRaft instructions for your Kafka distribution (ensure the broker is configured for KRaft controller mode). Confirm the broker is reachable at `localhost:9092`.

2) Create the required topics (optional if auto-create is enabled)

```bash
# Create topics using your Kafka distribution's kafka-topics tool (adjust path if needed)
kafka-topics --create --topic orders --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1
kafka-topics --create --topic orders-dlq --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1
```

3) Activate environment and install dependencies

```bash
# Use the provided venv (if present):
source bigdata/bin/activate

# Or create/activate your own venv:
# python3 -m venv venv
# source venv/bin/activate

pip install -r requirements.txt
```

4) Run the consumer (recommended before producing so it can process immediately)

```bash
python consumer.py
```

5) Produce sample messages

```bash
python producer.py
```

The `producer.py` will generate 15 orders with random prices between roughly 5.0 and 1500.0. Because of the configured failure rules a few messages may trigger transient failures or be sent to the DLQ.

6) Inspect DLQ messages

To read messages from the DLQ (raw bytes—if using `kafka-console-consumer`, you will see serialized bytes unless decoded):

```bash
kafka-console-consumer --topic orders-dlq --bootstrap-server localhost:9092 --from-beginning --property print.headers=true
```

If you're using this consumer script's logic, the DLQ messages include headers that explain the failure reason and original offsets.

## Expected behavior and notes

- Transient failures are simulated for prices between 5.0 and 50.0: the consumer will not commit offsets for transient failures so the message will be retried according to your consumer group's behavior and the script's retry logic.
- Permanent failures (price > 1000.0) are forwarded to `orders-dlq` with headers and are not reprocessed.
- The producer sets message keys to `orderId` to help partitioning/ordering.
- Both producer and consumer use Avro with the same `order.avsc` schema — ensure both sides use the same schema file.

## Troubleshooting

- "Unable to import confluent_kafka": ensure `confluent-kafka` is installed in your active Python environment. If you see compilation or wheel issues, install system dependencies like `librdkafka-dev` before reinstalling.
- Kafka connectivity errors: confirm the broker address and that Kafka is running. If using Docker, ensure correct port mappings.
- Avro deserialize errors: check that `order.avsc` matches the record structure produced by `producer.py`.

## Next steps (suggestions)

- Add a `docker-compose.yml` to stand up Kafka in KRaft mode for reproducible testing.
- Add automated tests for the producer and consumer (unit tests for serialize/deserialize and integration tests against a test Kafka broker).
- Add exponential backoff for retries and more granular failure categories.
- Add schema registry integration for centralized Avro schema management.

## License

This repository is provided as-is for learning and demo purposes.

